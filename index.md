---
layout: default
title: ML Theory References
---

# ML Theory References

# Current Research

## Journals

JMLR  
Mathematical Statistics and Learning (EMS press)

## Conferences

NeurIPS  
ICML  
COLT

# Statistical and machine learning resources

[https://realnotcomplex.com/probability-and-statistics/statistical-and-machine-learning](https://realnotcomplex.com/probability-and-statistics/statistical-and-machine-learning)

# Reinforcement Learning

UAlberta RL theory course.  
[https://rltheory.github.io/2024/](https://rltheory.github.io/2024/)

Reinforcement Learning: Theory and Algorithms, Agrawal et al.  
[https://rltheorybook.github.io/](https://rltheorybook.github.io/)

Distributional Reinforcement Learning (Bellemare et al.)  
[https://www.distributional-rl.org/](https://www.distributional-rl.org/)

Amir-massoud Farahmand [Lecture Notes on Reinforcement Learning](https://amfarahmand.github.io/IntroRL/lectures/LNRL.pdf), 2021\.

Shie Mannor, Yishay Mansour and Aviv Tamar: Reinforcement Learning: Foundations, book in progress. February 2023 [https://rl-tau-2023.wdfiles.com/local--files/course-schedule/RL-book-short.pdf](https://rl-tau-2023.wdfiles.com/local--files/course-schedule/RL-book-short.pdf)   
Graduate level text that goes for breadth rather than depth.

Mathematical Foundations of Reinforcement Learning, Shiyu Zhao.

[https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning](https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning)

# Deep Learning Theory

The Modern Mathematics of Deep Learning, Berner et al.  
[https://arxiv.org/abs/2105.04026](https://arxiv.org/abs/2105.04026)

Deep learning: a statistical viewpoint, Bartlett et al.  
[https://www.cambridge.org/core/services/aop-cambridge-core/content/view/7BCB89D860CEDDD5726088FAD64F2A5A/S0962492921000027a.pdf/deep-learning-a-statistical-viewpoint.pdf](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/7BCB89D860CEDDD5726088FAD64F2A5A/S0962492921000027a.pdf/deep-learning-a-statistical-viewpoint.pdf)

Theory of Deep Learning, Arora et al.  
[https://pages.cs.wisc.edu/\~yliang/cs839\_spring22/material/DLbook.pdf](https://pages.cs.wisc.edu/~yliang/cs839_spring22/material/DLbook.pdf)

The Principles of Deep Learning Theory, Roberts et al.  
[https://deeplearningtheory.com/](https://deeplearningtheory.com/)

Understanding Deep Learning, Simon J.D. Prince.  
[https://udlbook.github.io/udlbook/](https://udlbook.github.io/udlbook/)

Deep learning theory lecture notes, Matus Telgarsky.  
[https://mjt.cs.illinois.edu/dlt/](https://mjt.cs.illinois.edu/dlt/)

Neural Network Learning, Martin Anthony and Peter Bartlett.  
[https://www.cambridge.org/core/books/neural-network-learning/665C8C7EB5E2ABC5367A55ADB04E2866](https://www.cambridge.org/core/books/neural-network-learning/665C8C7EB5E2ABC5367A55ADB04E2866)

# ML Theory

[Lecture Notes for Mathematics of Machine Learning (401-2684-00L at ETH Zurich) Afonso S. Bandeira & Nikita Zhivotovskiy ETH](https://metaphor.ethz.ch/x/2021/fs/401-2684-00L/sc/Math_of_ML_Lecture_Notes.pdf) 2021  
Csaba’s note: This is very nice; all arguments are kept quite elementary. The coverage is as follows:

1\. Introduction  
2\. Clustering and k-means   
3\. The Singular Value Decomposition   
4\. Low rank approximation of matrix data   
5\. Dimension Reduction and Principal Component Analysis   
6\. The Graph Laplacian   
7\. Cheeger Inequality and Spectral Clustering   
8\. Introduction to Finite Frame Theory   
9\. Parsimony  
10\. Compressed Sensing and Sparse Recovery   
11\. Low Coherence Frames   
12\. Matrix Completion & Recommendation Systems   
13\. Classification Theory: Finite Classes   
14\. PAC-Learning for infinite classes: stability and sample compression  
15\. Perceptron  
16\. Basic concentration inequalities  
17\. Uniform Convergence of Frequencies of Events to Their Probabilities  
18\. The Vapnik-Chervonenkis dimension  
19\. Classification with noise  
20\. Online Learning: Follow the Leader and Halving algorithms   
21\. Exponential Weights Algorithm   
22\. Introduction to (Stochastic) Gradient Descent

Unsupervised Learning and Data Parsimony:  
• Clustering and k-means  
• Singular Value Decomposition  
• Low Rank approximations and Eckart–Young–Mirsky Theorem  
• Dimension Reduction and Principal Component Analysis  
• Matrix Completion and the Netflix Prize  
• Overcomplete Dictionaries and Finite Frame Theory  
• Sparsity and Compressed Sensing  
• Introduction to Spectral Graph Theory.  
(2) Supervised and Online Learning:  
• Introduction to Classification and Generalization of Classifiers  
• Some concentration inequalities  
• Stability and VC Dimension  
• Online Learning: Learning with expert advice and exponential weights  
• A short introduction to Optimization and Gradient Descent

 It starts with various tidbits about unsupervised learning (clustering, matrix factorization, matrix completion and collaborative filtering, k-means), sparsity (frames), then covers basics of supervised learning (including sample compression), finishing with a light touch of online learning (sequential halving, exponential weights).

Mathematical Analysis of Machine Learning Algorithms, Tong Zhang.  
[https://tongzhang-ml.org/lt-book.html](https://tongzhang-ml.org/lt-book.html)

Understanding Machine Learning: From Theory to Algorithms, SSS.  
[https://www.cs.huji.ac.il/\~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf)

Learning Theory from First Principles, Francis Bach.  
[https://www.di.ens.fr/\~fbach/ltfp\_book.pdf](https://www.di.ens.fr/~fbach/ltfp_book.pdf)

Foundations of Machine Learning, Mohri et al.  
[https://cs.nyu.edu/\~mohri/mlbook/](https://cs.nyu.edu/~mohri/mlbook/)

Some links to ML theory books (Kathrik Sridharan’s collection for his course):  
[https://www.cs.cornell.edu/cOn the Vapnik-Chevonenkis-Sauer lemmaourses/cs6783/2021fa/reference.html](https://www.cs.cornell.edu/courses/cs6783/2021fa/reference.html)

Moritz Hardt and Benjamin Recht: PATTERNS, PREDICTIONS, AND ACTIONS. A story about machine learning.  
[https://mlstory.org/](https://mlstory.org/)

# Miscellaneous

Rob Nowak: Mathematical Foundations of Machine Learning, 2022, 154 pages long notes. Very good for the length\! [https://nowak.ece.wisc.edu/MFML.pdf](https://nowak.ece.wisc.edu/MFML.pdf) 

O’Donnell, Ryan. 2021\. “Analysis of Boolean Functions.” *arXiv \[cs.DM\]*. arXiv. [http://arxiv.org/abs/2105.10386](http://arxiv.org/abs/2105.10386).  
[https://www.cs.cmu.edu/\~odonnell/aobf12/](https://www.cs.cmu.edu/~odonnell/aobf12/) lecture videos etc.

Borkar, Vivek S., and K. S. Mallikarjuna Rao. Elementary Convexity with Optimization. Springer Nature Singapore 2023 [https://link.springer.com/book/10.1007/978-981-99-1652-8](https://link.springer.com/book/10.1007/978-981-99-1652-8) 

Sebastien Roch: Modern Discrete Probability: An Essential Toolkit, Cambridge University Press, in print 2023   
[https://people.math.wisc.edu/\~roch/mdp/index.html](https://people.math.wisc.edu/~roch/mdp/index.html) 

Introductory lectures on stochastic optimization, John Duchi.  
[https://web.stanford.edu/\~jduchi/PCMIConvex/Duchi16.pdf](https://web.stanford.edu/~jduchi/PCMIConvex/Duchi16.pdf)

A Modern Introduction to Online Learning, Francesco Orabonna.  
[https://arxiv.org/abs/1912.13213](https://arxiv.org/abs/1912.13213)

Competitive On-line Statistics, Volodya Vovk.  
[https://onlinelibrary.wiley.com/doi/10.1111/j.1751-5823.2001.tb00457.x](https://onlinelibrary.wiley.com/doi/10.1111/j.1751-5823.2001.tb00457.x)

Introduction to Online Nonstochastic Control, Hazan et al.  
[https://arxiv.org/abs/2211.09619](https://arxiv.org/abs/2211.09619)

Underactuated Robotics, Russ Tedrake.  
[https://underactuated.csail.mit.edu/index.html](https://underactuated.csail.mit.edu/index.html)  

Introduction to Online Convex Optimization, Elad Hazan.  
[https://arxiv.org/abs/1909.05207](https://arxiv.org/abs/1909.05207)

A First Course in Causal Inference, Peng Ding, U.C. Berkeley; stat-type approach. Starts with basics; suitable to undergrads. 478 pages\!  
[https://arxiv.org/abs/2305.18793](https://arxiv.org/abs/2305.18793)

Vadhan, Salil P. 2012\. “Pseudorandomness.” *Foundations and Trends in Theoretical Computer Science* 7 (1–3): 1–336.  
This is just such a beautiful topic. [https://people.seas.harvard.edu/\~salil/pseudorandomness/pseudorandomness-published-Dec12.pdf](https://people.seas.harvard.edu/~salil/pseudorandomness/pseudorandomness-published-Dec12.pdf) 

Robert M. Keller: Computer Science: Abstraction to Implementation, 2001  
Down to earth introduction to elementary concepts and ideas in computer science. Has a lot of breadth, and appropriate depth to get one started.  
[https://www.cs.hmc.edu/\~keller/cs60book/%20%20%20All.pdf](https://www.cs.hmc.edu/~keller/cs60book/%20%20%20All.pdf) 

Geffner, Bonet: A Concise Introduction to Models and Methods for Automated Planning, Synthesis Lectures on Artificial Intelligence and Machine Learning, 2013 [https://link.springer.com/book/10.1007/978-3-031-01564-9](https://link.springer.com/book/10.1007/978-3-031-01564-9)  
The scope is to introduce the basic concepts and ideas in classical planning, finishing with MDPs and POMDPs. Very concise, quite comprehensive and only perhaps 10 years behind the current SOTA. Not really a textbook, more like an extended literature review. For MDPs, no mention of function approximation, but the POMDP chapter covers relatively recent developments (MCTS). This is as good as it gets in 100 pages\!

Unorganized, from [Amir massoud Farahmand](mailto:farahmand@vectorinstitute.ai)’s [ML course](https://amfarahmand.github.io/IntroML-Fall2022/)

* (ESL) Trevor Hastie, Robert Tibshirani, and Jerome Friedman, [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/), 2009\.  
* (PRML) Christopher M. Bishop, [Pattern Recognition and Machine Learning](http://research.microsoft.com/en-us/um/people/cmbishop/PRML/index.htm), 2006\.  
* (RLIntro) Richard S. Sutton and Andrew G. Barto [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html), 2018\.  
* (LNRL) Amir-massoud Farahmand [Lecture Notes on Reinforcement Learning](https://amfarahmand.github.io/IntroRL/lectures/LNRL.pdf), 2021\. (draft)  
* (DL) Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016), [Deep Learning](http://www.deeplearningbook.org/)  
* (MLPP) Kevin P. Murphy, [Machine Learning: A Probabilistic Perspective](http://search.library.utoronto.ca/search?Ntx=mode%20matchallpartial&Ntk=Anywhere&N=0&Ntt=machine%20learning%20murphy&Nr=p_work_normalized:Murphy%20Kevin%20P%201970-%20Machine%20learning&uuid=26f5cd74-4ad9-48af-8409-8c8249bb4a39), 2013\.  
* (ISL) Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, [Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/), 2017\.  
* (SSBN) Shai Shalev-Shwartz and Shai Ben-David [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html), 2014\.  
* (ITIL) David MacKay, [Information Theory, Inference, and Learning Algorithms](http://www.inference.phy.cam.ac.uk/mackay/itila/book.html), 2003\.

Someone else’s collection on “Online Learning”; to be merged..?  
[https://sudeepraja.github.io/ResourceOnlineLearning/](https://sudeepraja.github.io/ResourceOnlineLearning/)

## Measure Theory, Probability Theory

Bogachev, V. I. 2010\. *Measure Theory Volumes I, II*. Springer.  
This is a pretty comprehensive reference. Everyone should get their hands on this one. 

Counterexamples in probability theory \[[link](https://www.dropbox.com/scl/fi/4gb8bquyeeyicv9box1ob/Dover-Books-on-Mathematics-Jordan-M.-Stoyanov-Counterexamples-in-Probability-Dover-Publications-2013.pdf?rlkey=ic71pdtc7edx2qbfckhmkyj8h&dl=0)\]

Tropp: “CMS/ACM 117: Probability Theory & Computational Mathematics.” 2024\. \[[pdf](https://tropp.caltech.edu/notes/Tro23-Probability-Theory-LN.pdf)\] [https://doi.org/10.7907/q75sz-e1e79](https://doi.org/10.7907/q75sz-e1e79) 

## Optimization

[\[2211.14103\] Conditional Gradient Methods](https://arxiv.org/abs/2211.14103)

## Computer science

Eugene L. Lawler: Combinatorial Optimization: Networks and Matroids. 1976\. \[[link](https://www.dropbox.com/scl/fi/qgxq7jr1wjisdl0jox9x3/Eugene-L.-Lawler-Combinatorial-Optimization_-Networks-and-Matroids-Oxford-University-Press-USA-1995-1.pdf?rlkey=udbgsq5c8h04cjyz28wjy918q&dl=0)\]  
A classic. Matroids. Graphs. All the basic stuff. Written beautifully.

Grötschel, Martin, Laszlo Lovasz, and Alexander Schrijver. 2011\. Geometric Algorithms and Combinatorial Optimization. Springer Berlin Heidelberg.  
Uniquely mixing computing science, with continuous,discrete math.  
*Learn about computational complexity & numerical algorithms, polyhedra/convexity/convex optimization, the ellipsoid method, and applications to combinatorial optimization. Beautifully written\! One of my favorites*  
